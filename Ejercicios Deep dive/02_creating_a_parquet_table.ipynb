{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce836498-983b-4190-b994-4e631b104bb8"}}},{"cell_type":"markdown","source":["# Create a Parquet Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85a56fd6-ac07-4b53-adb7-e7a476614b74"}}},{"cell_type":"markdown","source":["## Notebook Configuration\n\nBefore you run this cell, make sure to add a unique user name to the file\n<a href=\"$./includes/configuration\" target=\"_blank\">\nincludes/configuration</a>, e.g.\n\n```username = \"yourfirstname_yourlastname\"```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd601a07-5179-4a1c-bab5-c7e6a720c7ce"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ff21b05-dfb0-4aa5-bcad-6be9b0086a88"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Reload data to DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c36e041-cabe-4c25-bfc6-001a12a302c2"}}},{"cell_type":"code","source":["file_path = health_tracker + \"raw/health_tracker_data_2020_1.json\"\nhealth_tracker_data_2020_1_df = spark.read.format(\"json\").load(file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"435e0185-504f-4d14-aacd-9f09da76f8ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create a Parquet Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e713b882-096b-4b20-83c6-ae954bae38e8"}}},{"cell_type":"markdown","source":["#### Step 1: Make Idempotent\nFirst, we remove the files in the `healthtracker/processed` directory.\n\nThen, we drop the table we will create from the Metastore if it exists.\n\nThis step will make the notebook idempotent. In other words, it could be run more than once without throwing errors or introducing extra files.\n\nðŸš¨ **NOTE** Throughout this lesson, we'll be writing files to the root location of the Databricks File System (DBFS). In general, best practice is to write files to your cloud object storage. We use DBFS root here for demonstration purposes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d50d0b27-92ed-45de-8e01-817962b05565"}}},{"cell_type":"code","source":["dbutils.fs.rm(health_tracker + \"processed\", recurse=True)\n\nspark.sql(\n    f\"\"\"\nDROP TABLE IF EXISTS health_tracker_processed\n\"\"\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d69c7e1b-59db-4f5c-9f89-2c5a278dc7d6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Transform the Data\nWe perform transformations by selecting columns in the following ways:\n- use `from_unixtime` to transform `\"time\"`, cast as a `date`, and aliased to `dte`\n- use `from_unixtime` to transform `\"time\"`, cast as a `timestamp`, and aliased to `time`\n- `heartrate` is selected as is\n- `name` is selected as is\n- cast `\"device_id\"` as an integer aliased to `p_device_id`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c896c82c-cdfc-41a7-af72-16a6c5d1ed79"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, from_unixtime\n\n\ndef process_health_tracker_data(dataframe):\n    return dataframe.select(\n        from_unixtime(\"time\").cast(\"date\").alias(\"dte\"),\n        from_unixtime(\"time\").cast(\"timestamp\").alias(\"time\"),\n        \"heartrate\",\n        \"name\",\n        col(\"device_id\").cast(\"integer\").alias(\"p_device_id\"),\n    )\n\n\nprocessedDF = process_health_tracker_data(health_tracker_data_2020_1_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"167c5f55-af4d-486c-9ff0-eec59a7cd41e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: Write the Files to the processed directory\nNote that we are partitioning the data by device id.\n\n1. use `.format(\"parquet\")`\n1. partition by `\"p_device_id\"`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3405c767-97c1-4a2b-a5ff-e02dcd8a25c4"}}},{"cell_type":"code","source":["# ANSWER\n(\n    processedDF.write.mode(\"overwrite\")\n    .format(\"parquet\")\n    .partitionBy(\"p_device_id\")\n    .save(health_tracker + \"processed\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"705e44ee-2e3f-47db-add1-c4f2e8fe978b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 4: Register the Table in the Metastore\nNext, use Spark SQL to register the table in the metastore.\nUpon creation we specify the format as parquet and that the location where the parquet files were written should be used."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0980e56-ff30-4db3-bdd0-5d2acd71b693"}}},{"cell_type":"code","source":["spark.sql(\n    f\"\"\"\nDROP TABLE IF EXISTS health_tracker_processed\n\"\"\"\n)\n\nspark.sql(\n    f\"\"\"\nCREATE TABLE health_tracker_processed\nUSING PARQUET\nLOCATION \"{health_tracker}/processed\"\n\"\"\"\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce0702db-11da-4eb3-8c99-93f8e36d32bf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 5: Verify Parquet-based Data Lake table\n\nCount the records in the `health_tracker_processed` Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ea1fd97-2ad2-4f9a-bf8f-53d23b07cde3"}}},{"cell_type":"code","source":["# ANSWER\nhealth_tracker_processed = spark.read.table(\"health_tracker_processed\")\nhealth_tracker_processed.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5dde26e-e716-4500-8b9d-e7381963efa0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the count does not return results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e0da336-ff7c-412f-967d-357e58a14b71"}}},{"cell_type":"markdown","source":["#### Step 6: Register the Partitions\n\nPer best practice, we have created a partitioned table. However, if you create a partitioned table from existing data,\nSpark SQL does not automatically discover the partitions and register them in the Metastore.\n\n`MSCK REPAIR TABLE` will register the partitions in the Hive Metastore. Learn more about this command in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-ddl-repair-table.html\" target=\"_blank\">\nthe docs</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d420c3e-96b8-425a-9863-da724fe7847b"}}},{"cell_type":"code","source":["spark.sql(\"MSCK REPAIR TABLE health_tracker_processed\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eeab5ed2-956b-498e-a3fa-8d16a75e8132"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 7: Count the Records in the `health_tracker_processed` table\n\nCount the records in the `health_tracker_processed` table.\n\nWith the table repaired and the partitions registered, we now have results.\nWe expect there to be 3720 records: five device measurements, 24 hours a day for 31 days."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b4e9b11-470d-4fe6-9a6d-4507c05d3467"}}},{"cell_type":"code","source":["# ANSWER\nhealth_tracker_processed.count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24f4af33-9850-4f1e-a540-4e541a7c565c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14c6a0b1-300b-4163-80f5-7ce8fb4d462c"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02_creating_a_parquet_table","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":434284992484352}},"nbformat":4,"nbformat_minor":0}
