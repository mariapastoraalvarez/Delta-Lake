{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d0d6b8d-ef84-413e-8c7f-721e9301c59b"}}},{"cell_type":"markdown","source":["# Schema Enforcement & Evolution\n\n**Objective:** Work with evolving schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e3fdd72-cf15-4688-a8c2-c4d8bf30ee88"}}},{"cell_type":"markdown","source":["## Notebook Configuration\n\nBefore you run this cell, make sure to add a unique user name to the file\n<a href=\"$./includes/configuration\" target=\"_blank\">\nincludes/configuration</a>, e.g.\n\n```\nusername = \"yourfirstname_yourlastname\"\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a28b3503-c67f-4d33-9d78-b139de3dcdd0"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23d8d722-d8bb-4e0e-aa96-63bbd61a9b44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Health tracker data sample\n\n```\n{\"device_id\":0,\"heartrate\":57.6447293596,\"name\":\"Deborah Powell\",\"time\":1.5830208E9,\"device_type\":\"version 2\"}\n{\"device_id\":0,\"heartrate\":57.6175546013,\"name\":\"Deborah Powell\",\"time\":1.5830244E9,\"device_type\":\"version 2\"}\n{\"device_id\":0,\"heartrate\":57.8486376876,\"name\":\"Deborah Powell\",\"time\":1.583028E9,\"device_type\":\"version 2\"}\n{\"device_id\":0,\"heartrate\":57.8821378637,\"name\":\"Deborah Powell\",\"time\":1.5830316E9,\"device_type\":\"version 2\"}\n{\"device_id\":0,\"heartrate\":59.0531490807,\"name\":\"Deborah Powell\",\"time\":1.5830352E9,\"device_type\":\"version 2\"}\n```\nThis shows a sample of the health tracker data we will be using. Note that each line is a valid JSON object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aecbd033-1bea-4ccb-b50b-53cb3613ce93"}}},{"cell_type":"markdown","source":["### Health tracker data schema\nThe data has the following schema:\n\n\n| Column     | Type      |\n|------------|-----------|\n| name       | string    |\n| heartrate  | double    |\n| device_id  | int       |\n| time       | long      |\n| device_type| string    |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b998c734-a90c-472d-b13a-2cc068f9d4d8"}}},{"cell_type":"markdown","source":["### Step 1: Load the Next Month of Data\nWe begin by loading the data from the file `health_tracker_data_2020_3.json`, using the `.format(\"json\")` option as before."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62db2def-50af-423c-9bb6-d48e956a8590"}}},{"cell_type":"code","source":["file_path = health_tracker + \"raw/health_tracker_data_2020_3.json\"\n\n\nhealth_tracker_data_2020_3_df = (\n  spark.read\n  .format(\"json\")\n  .load(file_path)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"819b5ed2-c496-48b7-924b-ba8a9be03033"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 2: Transform the Data\n\nWe perform the same data engineering on the data:\n- Use the `from_unixtime` Spark SQL function to transform the unixtime into a time string\n- Cast the time column to type `timestamp` to replace the column `time`\n- Cast the time column to type `date` to create the column `dte`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6744c390-415e-440b-b106-b45be6c8d911"}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, from_unixtime\ndef process_health_tracker_data(dataframe):\n    return (\n     dataframe\n     .select(\n         from_unixtime(\"time\").cast(\"date\").alias(\"dte\"),\n         from_unixtime(\"time\").cast(\"timestamp\").alias(\"time\"),\n         \"heartrate\",\n         \"name\",\n         col(\"device_id\").cast(\"integer\").alias(\"p_device_id\"),\n         \"device_type\"\n       )\n     )\nprocessedDF = process_health_tracker_data(health_tracker_data_2020_3_df)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16075b47-1fb2-45b8-8452-940ef3a65474"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 3: Append the Data to the `health_tracker_processed` Delta table\nWe do this using `.mode(\"append\")`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c26657bc-540e-4f9b-ab1b-c4c95679e970"}}},{"cell_type":"code","source":["from pyspark.sql.utils import AnalysisException\nfrom pyspark.sql.functions import lit\n\ntry:\n  (\n    processedDF.write\n    .mode(\"append\")\n    .format(\"delta\")\n    .save(health_tracker + \"processed\")\n  )\nexcept AnalysisException as error:\n  print(\"Analysis Exception:\")\n  print(error)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c2c07a3-709b-48f4-9f5b-4d1f0b306317"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Schema Mismatch\nThe command above produces the error: \n```\nAnalysisException: A schema mismatch detected when writing to the Delta table (Table ID: ...)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e01c248a-ef06-4ace-ba2b-c86a0b6d9f2b"}}},{"cell_type":"markdown","source":["To enable schema migration using DataFrameWriter or DataStreamWriter, set: `.option(\"mergeSchema\", \"true\")`.\n\nFor other operations, set the session configuration `spark.databricks.delta.schema.autoMerge.enabled` to `\"true\"`. See [the documentation](https://databricks.com/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html) specific to the operation for details."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eabd39f8-2e6b-4ec0-8295-a8bf8238f5b4"}}},{"cell_type":"markdown","source":["## What Is Schema Enforcement?\nSchema enforcement, also known as schema validation, is a safeguard in Delta Lake that ensures data quality by rejecting writes to a table that do not match the table’s schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"500a896f-6865-46ba-9403-650da1b949e4"}}},{"cell_type":"markdown","source":["## What Is Schema Evolution?\n\nSchema evolution is a feature that allows users to easily change a table’s current schema to accommodate data that is changing over time. Most commonly, it’s used when performing an append or overwrite operation, to automatically adapt the schema to include one or more new columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d00482c6-e6da-4c46-9eb4-298ffdca3529"}}},{"cell_type":"markdown","source":["#### Step 1: Append the Data with Schema Evolution to the `health_tracker_processed` Delta table\nWe do this using `.mode(\"append\")`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e73e2858-b78e-45bb-8dce-d132b61c8980"}}},{"cell_type":"code","source":["# ANSWER\n(processedDF.write\n .mode(\"append\")\n .option(\"mergeSchema\", True)\n .format(\"delta\")\n .save(health_tracker + \"processed\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8b08586-421a-4044-8dff-3b254743161f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Verify the Commit\n### Step 1: Count the Most Recent Version"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87d28e96-114d-423a-94db-cd5a7b780f01"}}},{"cell_type":"code","source":["spark.read.table(\"health_tracker_processed\").count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc18f304-6e53-4e94-8fc5-5d1984006807"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b61f205d-05ea-4eb1-b9c2-4f8e6f93b2e1"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"07_schema_enforcement_evolution","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":434284992484220}},"nbformat":4,"nbformat_minor":0}
