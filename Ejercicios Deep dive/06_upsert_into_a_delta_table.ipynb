{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dcd6c9e-642a-458b-9adb-3b86f1ea65b0"}}},{"cell_type":"markdown","source":["# Upsert Into a Delta Table\n\n**Objective:**  Repair records with an upsert\n\nIn the previous lesson, we identified two issues with the `health_tracker_processed` table:\n- There were 72 missing records\n- There were 60 records with broken readings\n\nIn this lesson, we will repair the table by modifying the `health_tracker_processed` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7474e72-7378-40bb-89b7-f0ee31f8ef42"}}},{"cell_type":"markdown","source":["## Notebook Configuration\n\nBefore you run this cell, make sure to add a unique user name to the file\n<a href=\"$./includes/configuration\" target=\"_blank\">\nincludes/configuration</a>, e.g.\n\n```\nusername = \"yourfirstname_yourlastname\"\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a14844b-9378-4e19-86b2-af7b25418316"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bfcc946-e3b5-4d69-96a8-304bba2ec479"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./includes/main/python/operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"706039f5-d817-4679-8f0b-4ad1adfaba1b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Prepare Updates DataFrame\nTo repair the broken sensor readings (less than zero), we'll interpolate using the value recorded before and after for each device. The Spark SQL functions LAG and LEAD will make this a trivial calculation.\nWe'll write these values to a temporary view called updates. This view will be used later to upsert values into our health_tracker_processed Delta table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69928841-97cc-47eb-89f2-b33731cf4171"}}},{"cell_type":"markdown","source":["#### Step 1: Create a DataFrame Interpolating Broken Values\nRecall that we want to partition on our Device ID column, which we named:\n`\"p_device_id\"`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d2be40b-82b0-45f9-be31-d2ed06c265b1"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, lag, lead\n\ndteWindow = Window.partitionBy(\"p_device_id\").orderBy(\"dte\")\n\ninterpolatedDF = (\n   spark.read\n   .table(\"health_tracker_processed\")\n   .select(col(\"dte\"),\n           col(\"time\"),\n           col(\"heartrate\"),\n           lag(col(\"heartrate\")).over(dteWindow).alias(\"prev_amt\"),\n           lead(col(\"heartrate\")).over(dteWindow).alias(\"next_amt\"),\n           col(\"name\"),\n           col(\"p_device_id\"))\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a5fd648-3a32-474c-b76b-75c3d817438d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Create a DataFrame of Updates"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9d7044e-7d06-4f05-b609-e92dd2d658c7"}}},{"cell_type":"code","source":["updatesDF = (\n  interpolatedDF\n  .where(col(\"heartrate\") < 0)\n  .select(col(\"dte\"),\n          col(\"time\"),\n          ((col(\"prev_amt\") + col(\"next_amt\"))/2).alias(\"heartrate\"),\n          col(\"name\"),\n          col(\"p_device_id\"))\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7d32d9e-95a1-46df-8aa9-af5e2c0c6d34"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: View the schemas of the `updatesDF` and `health_tracker_processed` table\nWe use the `.printSchema()` function to view the schema of the `health_tracker_processed` table.\n\nFill in the format we should use and run the cell below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de751065-cc0e-4fb5-ae23-d91dff44dff8"}}},{"cell_type":"code","source":["# ANSWER\n(\n  spark.read\n  .format(\"delta\")\n  .load(health_tracker + \"processed\")\n  .printSchema()\n)\nupdatesDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aae41f0-8303-413d-91c3-f2d79834479a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 4: Verify `updatesDF`\nPerform a `.count()` on the `updatesDF` view. It should have the same number of records as the `SUM` performed on the broken_readings view."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46bc6276-c7a3-40ce-950d-7c3c9d31a4c8"}}},{"cell_type":"code","source":["updatesDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8806779a-f7de-4cdd-8ac9-a9e2dc2984d2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Prepare Inserts DataFrame\nIt turns out that our expectation of receiving the missing records late was correct. These records have subsequently been made available to us as the file `health_tracker_data_2020_02_01.json`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"793cb8be-9ebe-4767-8aac-afce30a7480d"}}},{"cell_type":"markdown","source":["#### Step 1: Load the Late-Arriving Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef5ecd10-bc3c-4a1c-b79b-0522c3a5534a"}}},{"cell_type":"code","source":["file_path = health_tracker + \"raw/late/health_tracker_data_2020_2_late.json\"\n\nhealth_tracker_data_2020_2_late_df = (\n  spark.read\n  .format(\"json\")\n  .load(file_path)\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abbab02c-91e0-454e-94eb-0b5d90fcc238"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ANSWER\nhealth_tracker_data_2020_2_late_df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a06a6ab-65c1-4241-ae36-d10ddc2a652b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Transform the Data\nIn addition to updating the broken records, we wish to add this late-arriving data. We begin by preparing another temporary view with the appropriate transformations:\n* Use the `from_unixtime` Spark SQL function to transform the unixtime into a time string\n* Cast the `time` column to type `timestamp` to replace the column `time`\n* Cast the `time` column to type `date` to create the column `dte`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac3d0c4f-0050-4204-909d-545cfc0406ad"}}},{"cell_type":"code","source":["insertsDF = process_health_tracker_data(spark, health_tracker_data_2020_2_late_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c137cf46-5337-4132-9b23-453bb54cb136"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: View the Schema of the Inserts DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cee016e-c1dd-4986-af40-3703f1ee4528"}}},{"cell_type":"code","source":["insertsDF.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6366eb36-5b2b-434a-b6f3-2c5ae068b1d3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80bba964-e73b-44e3-8150-0ec513150c04"}}},{"cell_type":"markdown","source":["#### Step 1: Create the Union DataFrame\nFinally, we prepare the `upsertsDF` that consists of all the records in both the `updatesDF` and the `insertsDF`. We use the DataFrame `.union()` command to create the view."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b12dead-6d33-45b4-8e24-2130474188eb"}}},{"cell_type":"code","source":["upsertsDF = updatesDF.union(insertsDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e7f30dc-5017-40d0-8a0d-ece87a81da34"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: View the Schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1970953-c313-4290-b98f-a7d9ac49748d"}}},{"cell_type":"code","source":["upsertsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"741d0152-78f7-4410-91a6-5013c080199d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Perform Upsert Into the `health_tracker_processed` Table\nYou can upsert data into a Delta table using the merge operation. This operation is similar to the SQL `MERGE` command but has added support for deletes and other conditions in updates, inserts, and deletes. In other words, using the DeltaTable command `.merge()` provides full support for an upsert operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4238a92e-42aa-4519-8ae6-1feb4d3a4aaa"}}},{"cell_type":"markdown","source":["#### Step 1: Perform the Upsert"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dd2d1d7-52f2-407e-94b7-2b54065c00a3"}}},{"cell_type":"code","source":["from delta.tables import DeltaTable\nprocessedDeltaTable = DeltaTable.forPath(spark, health_tracker + \"processed\")\n\nupdate_match = \"\"\"health_tracker.time = upserts.time\n                  AND\n                  health_tracker.p_device_id = upserts.p_device_id\"\"\"\nupdate = {\"heartrate\" : \"upserts.heartrate\"}\n\ninsert = {\n    \"p_device_id\" : \"upserts.p_device_id\",\n    \"heartrate\" : \"upserts.heartrate\",\n    \"name\" : \"upserts.name\",\n    \"time\" : \"upserts.time\",\n    \"dte\" : \"upserts.dte\"\n}\n\n(processedDeltaTable.alias(\"health_tracker\")\n .merge(upsertsDF.alias(\"upserts\"), update_match)\n .whenMatchedUpdate(set=update)\n .whenNotMatchedInsert(values=insert)\n .execute())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63ce2c54-2138-4d2c-8916-b787734a48f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## View the Commit Using Time Travel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffc38e44-cf8a-42a8-b6c7-1ac92c0a0f31"}}},{"cell_type":"markdown","source":["#### Step 1: View the table as of Version 2\nThis is done by specifying the option `\"versionAsOf\"` as 2. When we time travel to Version 0, we see only the first month of data. In version 1, we see the table after we added comments. \nWhen we time travel to Version 2, we see the first two months of data, minus the 72 missing records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e49093d-2de1-42cb-ab5f-0d237239cefa"}}},{"cell_type":"code","source":["(spark.read\n .option(\"versionAsOf\", 2)\n .format(\"delta\")\n .load(health_tracker + \"processed\")\n .count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c08fcc5e-6564-4a99-bc3a-5bbcb4b8ddee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Count the Most Recent Version\nWhen we query the table without specifying a version, it shows the latest version of the table and includes the full two months of data.\nNote that the range of data includes the month of February during a leap year. That is why there are 29 days in the month."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61cd6677-28e1-4733-9962-c411cbee881b"}}},{"cell_type":"code","source":["spark.read.table(\"health_tracker_processed\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c2be273-3c50-4451-a4b6-13a8cca08314"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: Describe the History of the `health_tracker_processed` Table\nThe `.history()` Delta Table command provides provenance information, including the operation, user, and so on, for each write to a table.\nNote that each operation performed on the table is given a version number. These are the numbers we have been using when performing a time travel query on the table, e.g., `SELECT COUNT(*) FROM health_tracker_processed VERSION AS OF 1`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d910377-dc0c-4212-a198-6e35d5e203a2"}}},{"cell_type":"code","source":["display(processedDeltaTable.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79fbe892-3d4d-4719-963d-e12f46c709f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Perform a Second Upsert\nIn the previous lesson, we performed an upsert to the `health_tracker_processed` table, which updated records containing broken readings. When we inserted the late arriving data, we inadvertently added more broken readings!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8020995-d7d6-4e7b-a339-f7eb4fcc206a"}}},{"cell_type":"markdown","source":["#### Step 1: Sum the Broken Readings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"777c9188-b8a1-4c1a-b537-40918914ec04"}}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import col, count\n\nbroken_readings = (\n  spark.read\n  .format(\"delta\")\n  .load(health_tracker + \"processed\")\n  .select(col(\"heartrate\"), col(\"dte\"))\n  .where(col(\"heartrate\") < 0)\n  .groupby(\"dte\")\n  .agg(count(\"heartrate\"))\n  .orderBy(\"dte\")\n)\nbroken_readings.createOrReplaceTempView(\"broken_readings\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b3ebd9a-6091-45d9-b8e9-32469cf305b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Verify That These are New Broken Readings\nLet’s query the broken_readings with a `WHERE` clause to verify that these are indeed new broken readings introduced by inserting the late-arriving data.\nNote that there are no broken readings before ‘2020-02-25’."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9053ad47-8144-45be-80e8-eb4d84c809b5"}}},{"cell_type":"code","source":["%sql\nSELECT SUM(`count(heartrate)`) FROM broken_readings WHERE dte < '2020-02-25'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d24bf7d2-dae9-4d85-a8ee-2f719d09c883"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: Verify Updates\nPerform a `.count()` on the `updatesDF` view.\n\n**Note:** It is not necessary to redefine the DataFrame. Recall that a Spark DataFrame is lazily defined, pulling the correct number of updates when an action is triggered.\nIt should have the same number of records as the SUM performed on the `broken_readings` view."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d57f4f76-6e18-4b52-b587-b80bdc3e3388"}}},{"cell_type":"code","source":["# ANSWER\nupdatesDF.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c35c1a24-3f57-4bdb-bcbd-d5e9cbb127a1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 4: Perform Upsert Into the `health_tracker_processed` Table\nOnce more, we upsert into the `health_tracker_processed` table using the DeltaTable command `.merge()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96cf108d-cdb1-4f63-b12a-ddb53ded549a"}}},{"cell_type":"code","source":["upsertsDF = updatesDF\n\n(processedDeltaTable.alias(\"health_tracker\")\n .merge(upsertsDF.alias(\"upserts\"), update_match)\n .whenMatchedUpdate(set=update)\n .whenNotMatchedInsert(values=insert)\n .execute())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b650fac1-7ddc-4db8-9d5c-3ce1e9705d45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 5: Sum the Broken Readings\nLet’s sum the records in the `broken_readings` view one last time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14a6d5e0-fd1a-4d8e-b2b6-cb4d8429c1c5"}}},{"cell_type":"code","source":["%sql\nSELECT SUM(`count(heartrate)`) FROM broken_readings\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55218491-483b-4a12-845f-79a1c7864a06"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e1062ca-56a0-4e50-9dc0-6f61a6aa675a"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"06_upsert_into_a_delta_table","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":434284992484284}},"nbformat":4,"nbformat_minor":0}
