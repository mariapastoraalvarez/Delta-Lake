{"cells":[{"cell_type":"markdown","source":["d-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94e3d46a-7fcd-4107-ad23-678a0cb39559"}}},{"cell_type":"markdown","source":["# Batch Write to Delta Tables\n\n**Objective:** Append files to an existing Delta Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19e27c96-a682-414e-bdf0-14a5c4fc5e73"}}},{"cell_type":"markdown","source":["## Notebook Configuration\n\nBefore you run this cell, make sure to add a unique user name to the file\n<a href=\"$./includes/configuration\" target=\"_blank\">\nincludes/configuration</a>, e.g.\n\n```\nusername = \"yourfirstname_yourlastname\"\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"424c1116-2aa5-4959-a101-5f8b283f6736"}}},{"cell_type":"code","source":["%run ./includes/configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b5f753f-c1c0-499d-8a04-7f57c71ce8db"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%run ./includes/main/python/operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5578562c-9d51-43b3-9452-790ef76ab533"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 1: Load New Data\n\nWithin the context of our data ingestion pipeline, this is the addition of new raw files to our Single Source of Truth.\n\nWe begin by loading the data from the file `health_tracker_data_2020_2.json`, using the `.format(\"json\")` option as before."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cc7d628-b2e4-4765-96c3-81042da4170e"}}},{"cell_type":"code","source":["file_path = health_tracker + \"raw/health_tracker_data_2020_2.json\"\n\nhealth_tracker_data_2020_2_df = (\n  spark.read\n  .format(\"json\")\n  .load(file_path)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9afee73b-0fbd-4ee2-a1eb-cb666b29ede6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Transform the Data\nWe perform the same data engineering on the data:\n- Use the from_unixtime Spark SQL function to transform the unixtime into a time string\n- Cast the time column to type timestamp to replace the column time\n- Cast the time column to type date to create the column dte"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5590bc06-6cb3-4d62-8504-7627becab4d1"}}},{"cell_type":"code","source":["processedDF = process_health_tracker_data(spark, health_tracker_data_2020_2_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9f05644-1f0c-491e-bf31-b80b3193e482"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 3: Append the Data to the `health_tracker_processed` Delta table\nWe do this using `.mode(\"append\")`. Note that it is not necessary to perform any action on the Metastore."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4743ef7-dc4c-493e-8967-b319c6958d18"}}},{"cell_type":"code","source":["(processedDF.write\n .mode(\"append\")\n .format(\"delta\")\n .save(health_tracker + \"processed\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c564d7cb-7b74-4d65-aff5-942d8deb60ab"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### View the Commit Using Time Travel\nDelta Lake can query an earlier version of a Delta table using a feature known as time travel. Here, we query the data as of version 0, that is, the initial conversion of the table from Parquet."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"715b0720-4758-4a79-9d0f-0fe9d6fa8561"}}},{"cell_type":"markdown","source":["#### Step 1: View the table as of Version 0\nThis is done by specifying the option `\"versionAsOf\"` as 0. When we time travel to Version 0, we see **only** the first month of data, five device measurements, 24 hours a day for 31 days."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e39a815a-0f48-4479-ac1b-2a1c576ac602"}}},{"cell_type":"code","source":["(spark.read\n .option(\"versionAsOf\", 0)\n .format(\"delta\")\n .load(health_tracker + \"processed\")\n .count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"028c2079-4418-4926-8a59-c7e9a59513cb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Step 2: Count the Most Recent Version\nWhen we query the table without specifying a version, it shows the latest version of the table and includes the new records added.\nWhen we look at the current version, we expect to see two months of data: January 2020 and February 2020. \n\nThe data should include the following records: \n\n``` 5 devices * 60 days * 24 hours = 7200 records```\n\nNote that the range of data includes the month of February during a leap year. 29 days in Feb plus 31 in January gives us 60 days total."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5adee3e1-897b-4f3f-a650-5605b6f17fc6"}}},{"cell_type":"code","source":["(spark.read\n .format(\"delta\")\n .load(health_tracker + \"processed\")\n .count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f9e29c9-0d7a-4d5a-9baa-df806a04ce52"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we do not have a correct count. We are missing 72 records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67297b92-a96f-4d65-af7a-0d26c9584bb9"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d02076c1-9746-4341-af9a-36a0c51f2067"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04_batch_write_to_delta_tables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":434284992484392}},"nbformat":4,"nbformat_minor":0}
